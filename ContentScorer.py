"""
è‡ªå‹•æœå°‹ä¸¦è©•åˆ†ç¶²è·¯è³‡è¨Šç³»çµ±ï¼ˆå„ªåŒ–ç‰ˆè©•åˆ†é‚è¼¯ï¼‰
é©åˆä¸€èˆ¬ç¶²è·¯å…§å®¹çš„å¯¦ç”¨è©•åˆ†ç³»çµ±
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from datetime import datetime
import time
from typing import List, Dict
import warnings
from urllib.parse import urlparse, urljoin
warnings.filterwarnings('ignore')


class WebContentScorer:
    """ç¶²è·¯å…§å®¹æœå°‹èˆ‡è©•åˆ†ç³»çµ±ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
    
    def __init__(self, serpapi_key=None):
        """åˆå§‹åŒ–ç³»çµ±"""
        self.serpapi_key = "1af6b7da5496c681d01b3eeb8dda9635ee83817e66bfd355073f1596c3b366ff"
        self.results = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
        }
        
        # é»‘åå–®ç¶²åŸŸï¼ˆæ’é™¤ç¶­åŸºç™¾ç§‘ç­‰ï¼‰
        self.blacklist_domains = [
            'wikipedia.org',
            'wiki',
            'baike.baidu.com',
            'ç¶­åŸºç™¾ç§‘',
            'wikimedia'
        ]
        
    def search_google_serpapi(self, keyword: str, num_results: int = 10) -> List[Dict]:
        """ä½¿ç”¨ SerpAPI æœå°‹ Google"""
        if not self.serpapi_key:
            print("âš  æœªæä¾› SerpAPI å¯†é‘°ï¼Œå°‡ä½¿ç”¨å‚™ç”¨æœå°‹æ–¹æ³•")
            return self.search_duckduckgo(keyword, num_results)
        
        try:
            params = {
                "engine": "google",
                "q": keyword,
                "api_key": self.serpapi_key,
                "num": num_results * 2,  # å¤šæŠ“ä¸€äº›ï¼Œéæ¿¾å¾Œå¯èƒ½ä¸è¶³
                "hl": "zh-tw"
            }
            
            response = requests.get("https://serpapi.com/search", params=params, timeout=15)
            data = response.json()
            
            results = []
            for item in data.get("organic_results", []):
                url = item.get("link", "")
                
                # æª¢æŸ¥æ˜¯å¦åœ¨é»‘åå–®ä¸­
                if any(blocked in url.lower() for blocked in self.blacklist_domains):
                    print(f"   âŠ— å·²éæ¿¾é»‘åå–®ç¶²ç«™: {url}")
                    continue
                
                results.append({
                    "title": item.get("title", ""),
                    "url": url,
                    "snippet": item.get("snippet", ""),
                    "source": item.get("displayed_link", "")
                })
                
                if len(results) >= num_results:
                    break
            
            print(f"âœ“ ä½¿ç”¨ SerpAPI æ‰¾åˆ° {len(results)} ç­†çµæœï¼ˆå·²éæ¿¾é»‘åå–®ï¼‰")
            return results
            
        except Exception as e:
            print(f"âš  SerpAPI æœå°‹å¤±æ•—: {e}")
            return self.search_duckduckgo(keyword, num_results)
    
    def search_duckduckgo(self, keyword: str, num_results: int = 10) -> List[Dict]:
        """ä½¿ç”¨ DuckDuckGo æœå°‹ï¼ˆå…è²»æ–¹æ¡ˆï¼‰"""
        try:
            url = f"https://html.duckduckgo.com/html/?q={keyword}"
            response = requests.get(url, headers=self.headers, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            results = []
            for result in soup.find_all('div', class_='result'):
                title_tag = result.find('a', class_='result__a')
                snippet_tag = result.find('a', class_='result__snippet')
                
                if title_tag and title_tag.get('href'):
                    url = title_tag.get('href')
                    if 'uddg=' in url:
                        url = url.split('uddg=')[1].split('&')[0]
                    
                    # æª¢æŸ¥é»‘åå–®
                    if any(blocked in url.lower() for blocked in self.blacklist_domains):
                        continue
                    
                    results.append({
                        "title": title_tag.get_text(strip=True),
                        "url": url,
                        "snippet": snippet_tag.get_text(strip=True) if snippet_tag else "",
                        "source": urlparse(url).netloc
                    })
                    
                    if len(results) >= num_results:
                        break
            
            print(f"âœ“ ä½¿ç”¨ DuckDuckGo æ‰¾åˆ° {len(results)} ç­†çµæœ")
            return results
            
        except Exception as e:
            print(f"âŒ æœå°‹å¤±æ•—: {e}")
            return []
    
    def extract_content_advanced(self, url: str) -> Dict:
        """é€²éšå…§å®¹æŠ“å–æ–¹æ³•"""
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ¨™é¡Œ
            title = ""
            title_candidates = [
                soup.find('h1'),
                soup.find('title'),
                soup.find('meta', property='og:title'),
                soup.find('meta', attrs={'name': 'title'})
            ]
            for candidate in title_candidates:
                if candidate:
                    if candidate.name == 'meta':
                        title = candidate.get('content', '')
                    else:
                        title = candidate.get_text(strip=True)
                    if title:
                        break
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 
                           'aside', 'iframe', 'noscript', 'form']):
                tag.decompose()
            
            # æ‰¾ä¸»è¦å…§å®¹
            main_content = None
            content_selectors = [
                'article',
                '[role="main"]',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content',
                'main',
                '#content',
                '#main'
            ]
            
            for selector in content_selectors:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if not main_content:
                main_content = soup.find('body')
            
            if main_content:
                paragraphs = main_content.find_all(['p', 'div', 'span', 'li'])
                texts = []
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if 20 < len(text) < 1000:
                        texts.append(text)
                
                content = ' '.join(texts)
                content = re.sub(r'\s+', ' ', content).strip()
                
                # æå–ç™¼å¸ƒæ—¥æœŸ
                publish_date = None
                date_meta = soup.find('meta', property='article:published_time')
                if not date_meta:
                    date_meta = soup.find('meta', attrs={'name': 'date'})
                if date_meta:
                    publish_date = date_meta.get('content', '')
                
                return {
                    "content": content,
                    "title": title,
                    "publish_date": publish_date,
                    "method": "beautifulsoup_advanced",
                    "success": True
                }
            
            return {
                "content": "",
                "title": title,
                "publish_date": None,
                "method": "beautifulsoup_advanced",
                "success": False,
                "error": "æœªæ‰¾åˆ°ä¸»è¦å…§å®¹"
            }
            
        except requests.exceptions.Timeout:
            return {
                "content": "",
                "title": "",
                "publish_date": None,
                "method": "failed",
                "success": False,
                "error": "è«‹æ±‚è¶…æ™‚"
            }
        except Exception as e:
            return {
                "content": "",
                "title": "",
                "publish_date": None,
                "method": "failed",
                "success": False,
                "error": str(e)
            }
    
    def calculate_relevance_score(self, keyword: str, content: str, title: str, snippet: str = "") -> float:
        """
        è¨ˆç®—ç›¸é—œæ€§è©•åˆ†ï¼ˆå„ªåŒ–ç‰ˆï¼‰
        
        è©•åˆ†æ¨™æº–ï¼š
        - TF-IDF èªç¾©ç›¸ä¼¼åº¦ï¼š40åˆ†
        - æ¨™é¡Œå®Œå…¨åŒ¹é…ï¼š25åˆ†
        - æ¨™é¡Œéƒ¨åˆ†åŒ¹é…ï¼š15åˆ†
        - é—œéµè©é »ç‡åŠ åˆ†ï¼šæœ€å¤š20åˆ†
        - é—œéµè©ä½ç½®åŠ åˆ†ï¼ˆå‰æ®µï¼‰ï¼š15åˆ†
        - å°æ¨™é¡ŒåŒ…å«é—œéµè©ï¼š10åˆ†
        """
        if not content and not title:
            return 0.0
        
        try:
            # æº–å‚™æ–‡å­—ï¼ˆæ¨™é¡Œæ¬Šé‡æ›´é«˜ï¼‰
            weighted_content = f"{title} {title} {title} {snippet} {content}"
            keywords = keyword.split()
            
            # TF-IDF ç›¸ä¼¼åº¦ï¼ˆ40åˆ†ï¼‰
            vectorizer = TfidfVectorizer(token_pattern=r'(?u)\b\w+\b')
            tfidf_matrix = vectorizer.fit_transform([keyword, weighted_content])
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            base_score = similarity * 40
            
            content_lower = content.lower()
            title_lower = title.lower()
            keyword_lower = keyword.lower()
            
            # æ¨™é¡ŒåŒ¹é…åŠ åˆ†ï¼ˆ25åˆ†æˆ–15åˆ†ï¼‰
            if keyword_lower in title_lower:
                base_score += 25
            elif any(kw.lower() in title_lower for kw in keywords if len(kw) > 2):
                base_score += 15
            
            # é—œéµè©é »ç‡åŠ åˆ†ï¼ˆæœ€å¤š20åˆ†ï¼‰
            keyword_count = content_lower.count(keyword_lower)
            for kw in keywords:
                if len(kw) > 2:
                    keyword_count += content_lower.count(kw.lower()) * 0.5
            
            frequency_bonus = min(keyword_count * 2, 20)
            base_score += frequency_bonus
            
            # é—œéµè©ä½ç½®åŠ åˆ†ï¼ˆ15åˆ†ï¼‰
            # å‰500å­—å‡ºç¾ï¼š+15åˆ†
            # å‰1000å­—å‡ºç¾ï¼š+10åˆ†
            # å‰2000å­—å‡ºç¾ï¼š+5åˆ†
            if len(content_lower) > 0:
                if content_lower[:500].count(keyword_lower) > 0:
                    base_score += 15
                elif content_lower[:1000].count(keyword_lower) > 0:
                    base_score += 10
                elif content_lower[:2000].count(keyword_lower) > 0:
                    base_score += 5
            
            return min(base_score, 100)
            
        except Exception as e:
            print(f"   âš  è¨ˆç®—ç›¸é—œæ€§æ™‚å‡ºéŒ¯: {e}")
            return 0.0
    
    def calculate_quality_score(self, content: str, url: str, title: str, publish_date: str = None) -> float:
        """
        è¨ˆç®—å…§å®¹å“è³ªè©•åˆ†ï¼ˆå„ªåŒ–ç‰ˆ - é©åˆä¸€èˆ¬ç¶²è·¯å…§å®¹ï¼‰
        
        è©•åˆ†æ¨™æº–ï¼š
        - å…§å®¹æ·±åº¦ï¼ˆé•·åº¦+çµæ§‹ï¼‰ï¼š30åˆ†
        - å…§å®¹è±å¯Œåº¦ï¼ˆæ•¸å­—ã€åˆ—è¡¨ã€å¼•ç”¨ï¼‰ï¼š25åˆ†
        - ç¶²ç«™é¡å‹èˆ‡ä¿¡ä»»åº¦ï¼š20åˆ†
        - æ™‚æ•ˆæ€§ï¼ˆç™¼å¸ƒæ—¥æœŸï¼‰ï¼š15åˆ†
        - å¯è®€æ€§ï¼ˆæ®µè½ã€æ¨™é¡Œï¼‰ï¼š10åˆ†
        """
        score = 0
        
        # 1. å…§å®¹æ·±åº¦è©•åˆ†ï¼ˆ30åˆ†ï¼‰
        content_length = len(content)
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', content)
        valid_sentences = [s for s in sentences if len(s.strip()) > 10]
        
        # é•·åº¦è©•åˆ†ï¼ˆ15åˆ†ï¼‰
        if content_length > 3000:
            score += 15
        elif content_length > 2000:
            score += 12
        elif content_length > 1000:
            score += 9
        elif content_length > 500:
            score += 6
        elif content_length > 200:
            score += 3
        
        # çµæ§‹è©•åˆ†ï¼ˆ15åˆ†ï¼‰
        if len(valid_sentences) > 30:
            score += 15
        elif len(valid_sentences) > 20:
            score += 12
        elif len(valid_sentences) > 10:
            score += 8
        elif len(valid_sentences) > 5:
            score += 4
        
        # 2. å…§å®¹è±å¯Œåº¦è©•åˆ†ï¼ˆ25åˆ†ï¼‰
        # åŒ…å«æ•¸å­—/æ•¸æ“šï¼ˆ8åˆ†ï¼‰
        numbers_count = len(re.findall(r'\d+', content))
        if numbers_count > 10:
            score += 8
        elif numbers_count > 5:
            score += 5
        elif numbers_count > 0:
            score += 3
        
        # åŒ…å«å¼•ç”¨æˆ–å¼•è™Ÿï¼ˆ7åˆ†ï¼‰
        has_quotes = bool(re.search(r'["""\'\'ã€Œã€ã€ã€]', content))
        if has_quotes:
            score += 7
        
        # æ¨™é»ç¬¦è™Ÿè±å¯Œåº¦ï¼ˆè¡¨ç¤ºçµæ§‹åŒ–å…§å®¹ï¼‰ï¼ˆ10åˆ†ï¼‰
        punctuation_count = len(re.findall(r'[,ï¼Œã€;ï¼›:ï¼š]', content))
        if punctuation_count > 20:
            score += 10
        elif punctuation_count > 10:
            score += 6
        elif punctuation_count > 5:
            score += 3
        
        # 3. ç¶²ç«™é¡å‹èˆ‡ä¿¡ä»»åº¦è©•åˆ†ï¼ˆ20åˆ†ï¼‰
        url_lower = url.lower()
        
        # æ–°èåª’é«”ç¶²ç«™ï¼ˆ15åˆ†ï¼‰
        news_domains = [
            'news', 'bbc', 'cnn', 'nytimes', 'reuters', 'bloomberg',
            'guardian', 'washingtonpost', 'forbes', 'techcrunch',
            'theverge', 'wired', 'engadget', 'cnet'
        ]
        
        # å°ˆæ¥­ç¶²ç«™ï¼ˆ12åˆ†ï¼‰
        professional_domains = [
            '.gov', '.edu', '.org',
            'medium', 'github', 'stackoverflow',
            'arxiv', 'scholar', 'research'
        ]
        
        # ä¸€èˆ¬å¯ä¿¡ç¶²ç«™ï¼ˆ8åˆ†ï¼‰
        trusted_domains = [
            'https://',  # è‡³å°‘æœ‰ HTTPS
        ]
        
        if any(domain in url_lower for domain in news_domains):
            score += 15
        elif any(domain in url_lower for domain in professional_domains):
            score += 12
        elif url.startswith('https://'):
            score += 8
        else:
            score += 3  # åŸºæœ¬åˆ†
        
        # 4. æ™‚æ•ˆæ€§è©•åˆ†ï¼ˆ15åˆ†ï¼‰
        if publish_date:
            try:
                # å˜—è©¦è§£ææ—¥æœŸ
                date_formats = [
                    '%Y-%m-%dT%H:%M:%S',
                    '%Y-%m-%d',
                    '%Y/%m/%d'
                ]
                
                pub_date = None
                for fmt in date_formats:
                    try:
                        pub_date = datetime.strptime(publish_date[:19], fmt)
                        break
                    except:
                        continue
                
                if pub_date:
                    days_ago = (datetime.now() - pub_date).days
                    
                    if days_ago <= 7:
                        score += 15  # ä¸€é€±å…§
                    elif days_ago <= 30:
                        score += 12  # ä¸€å€‹æœˆå…§
                    elif days_ago <= 90:
                        score += 9   # ä¸‰å€‹æœˆå…§
                    elif days_ago <= 180:
                        score += 6   # åŠå¹´å…§
                    elif days_ago <= 365:
                        score += 3   # ä¸€å¹´å…§
                    else:
                        score += 1   # è¶…éä¸€å¹´
            except:
                score += 5  # æœ‰æ—¥æœŸä½†è§£æå¤±æ•—ï¼Œçµ¦ä¸­æ€§åˆ†
        else:
            score += 5  # ç„¡æ—¥æœŸè³‡è¨Šï¼Œçµ¦ä¸­æ€§åˆ†
        
        # 5. å¯è®€æ€§è©•åˆ†ï¼ˆ10åˆ†ï¼‰
        # æ¨™é¡Œå“è³ªï¼ˆ5åˆ†ï¼‰
        title_length = len(title)
        if 10 < title_length < 100:
            score += 5
        elif 5 < title_length < 150:
            score += 3
        
        # ç„¡åƒåœ¾å…§å®¹ï¼ˆ5åˆ†ï¼‰
        spam_keywords = [
            'é»æ“Šé€™è£¡', 'ç«‹å³è³¼è²·', 'å»£å‘Š', 'æ¨å»£', 'AD', 'è´ŠåŠ©',
            'é™æ™‚å„ªæƒ ', 'é¦¬ä¸Šæ¶è³¼', 'ç‰¹åƒ¹'
        ]
        spam_count = sum(1 for kw in spam_keywords if kw in content)
        score += max(5 - spam_count * 2, 0)
        
        return min(score, 100)
    
    def calculate_final_score(self, relevance: float, quality: float) -> float:
        """
        è¨ˆç®—æœ€çµ‚ç¶œåˆè©•åˆ†
        
        æ¬Šé‡åˆ†é…ï¼š
        - ç›¸é—œæ€§ï¼š60%ï¼ˆæ‰¾åˆ°å°çš„å…§å®¹æœ€é‡è¦ï¼‰
        - å“è³ªï¼š40%ï¼ˆå…§å®¹è¦æœ‰åƒ¹å€¼ï¼‰
        """
        final_score = (relevance * 0.60) + (quality * 0.40)
        return round(final_score, 2)
    
    def run(self, keyword: str, num_results: int = 10) -> pd.DataFrame:
        """åŸ·è¡Œå®Œæ•´çš„æœå°‹ã€æŠ“å–å’Œè©•åˆ†æµç¨‹"""
        print(f"\n{'='*70}")
        print(f"ğŸ” é–‹å§‹æœå°‹é—œéµè©: ã€Œ{keyword}ã€")
        print(f"{'='*70}\n")
        
        # æ­¥é©Ÿ 1: æœå°‹
        print("ğŸ“¡ æ­¥é©Ÿ 1/3: æ­£åœ¨æœå°‹...")
        search_results = self.search_google_serpapi(keyword, num_results)
        
        if not search_results:
            print("âŒ æœå°‹å¤±æ•—ï¼Œæœªæ‰¾åˆ°ä»»ä½•çµæœ")
            return pd.DataFrame()
        
        # æ­¥é©Ÿ 2: æŠ“å–å…§å®¹
        print(f"\nğŸ“¥ æ­¥é©Ÿ 2/3: æ­£åœ¨æŠ“å– {len(search_results)} å€‹ç¶²é å…§å®¹...")
        self.results = []
        
        for i, result in enumerate(search_results, 1):
            print(f"   [{i}/{len(search_results)}] {result['title'][:60]}...")
            
            # æŠ“å–å…§å®¹
            content_data = self.extract_content_advanced(result['url'])
            
            if not content_data['title']:
                content_data['title'] = result['title']
            
            # è¨ˆç®—è©•åˆ†
            if content_data['content'] and len(content_data['content']) > 100:
                relevance_score = self.calculate_relevance_score(
                    keyword, 
                    content_data['content'], 
                    content_data['title'],
                    result['snippet']
                )
                quality_score = self.calculate_quality_score(
                    content_data['content'],
                    result['url'],
                    content_data['title'],
                    content_data.get('publish_date')
                )
                final_score = self.calculate_final_score(relevance_score, quality_score)
                status = "âœ“"
            else:
                relevance_score = 0
                quality_score = 0
                final_score = 0
                status = "âœ—"
            
            print(f"       {status} ç»¼åˆè¯„åˆ†: {final_score:.1f} (ç›¸å…³æ€§: {relevance_score:.1f}, å“è´¨: {quality_score:.1f})")
            
            # ä¿å­˜çµæœ
            self.results.append({
                'æ’å': i,
                'æ ‡é¢˜': content_data['title'][:100],
                'æ¥æº': result['source'],
                'ç½‘å€': result['url'],
                'ç›¸å…³æ€§è¯„åˆ†': round(relevance_score, 1),
                'å“è´¨è¯„åˆ†': round(quality_score, 1),
                'ç»¼åˆè¯„åˆ†': final_score,
                'å†…å®¹é•¿åº¦': len(content_data['content']),
                'å‘å¸ƒæ—¥æœŸ': content_data.get('publish_date', 'N/A') or 'N/A',
                'æ‘˜è¦': result['snippet'][:150] + '...' if len(result['snippet']) > 150 else result['snippet'],
                'æŠ“å–çŠ¶æ€': 'æˆåŠŸ' if content_data.get('success') else 'å¤±æ•—'
            })
            
            time.sleep(0.8)
        
        # æ­¥é©Ÿ 3: æ’åºå’Œè¼¸å‡º
        print(f"\nğŸ“Š æ­¥é©Ÿ 3/3: æ­£åœ¨è©•åˆ†å’Œæ’åº...")
        df = pd.DataFrame(self.results)
        
        df = df.sort_values('ç»¼åˆè¯„åˆ†', ascending=False).reset_index(drop=True)
        df['æ’å'] = range(1, len(df) + 1)
        
        success_count = len(df[df['æŠ“å–çŠ¶æ€'] == 'æˆåŠŸ'])
        avg_score = df['ç»¼åˆè¯„åˆ†'].mean()
        
        print(f"\nâœ… å®Œæˆï¼")
        print(f"   â€¢ å…±è™•ç† {len(df)} ç­†è³‡æ–™")
        print(f"   â€¢ æˆåŠŸæŠ“å– {success_count} ç­†")
        print(f"   â€¢ å¹³å‡è©•åˆ† {avg_score:.1f}")
        print()
        
        return df
    
    def export_results(self, df: pd.DataFrame, filename: str = None):
        """åŒ¯å‡ºçµæœåˆ°æª”æ¡ˆ"""
        if df.empty:
            print("âš  æ²’æœ‰çµæœå¯ä»¥åŒ¯å‡º")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"æœå°‹çµæœ_{timestamp}.xlsx"
        
        try:
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"âœ“ çµæœå·²åŒ¯å‡ºåˆ°: {filename}")
        except ImportError:
            csv_filename = filename.replace('.xlsx', '.csv')
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"âœ“ çµæœå·²åŒ¯å‡ºåˆ°: {csv_filename}")
        except Exception as e:
            csv_filename = filename.replace('.xlsx', '.csv')
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"âœ“ çµæœå·²åŒ¯å‡ºåˆ°: {csv_filename}")


# ============================================================================
# ä½¿ç”¨ç¯„ä¾‹
# ============================================================================

def main():
    """ä¸»å‡½æ•¸"""
    
    print("="*70)
    print("  ç¶²è·¯å…§å®¹è‡ªå‹•æœå°‹èˆ‡è©•åˆ†ç³»çµ±ï¼ˆå„ªåŒ–ç‰ˆï¼‰")
    print("="*70)
    
    scorer = WebContentScorer()
    
    keyword = input("\nè«‹è¼¸å…¥æœå°‹é—œéµè©ï¼ˆç›´æ¥æŒ‰ Enter ä½¿ç”¨é è¨­ã€Œäººå·¥æ™ºæ…§ã€ï¼‰: ").strip()
    if not keyword:
        keyword = "äººå·¥æ™ºæ…§"
    
    num = input("è¦æœå°‹å¹¾ç­†è³‡æ–™ï¼Ÿ(é è¨­ 10): ").strip()
    num_results = int(num) if num.isdigit() else 10
    
    results_df = scorer.run(keyword, num_results=num_results)
    
    if not results_df.empty:
        print("\n" + "="*70)
        print("ğŸ“‹ æœå°‹çµæœåŒ¯ç¸½")
        print("="*70 + "\n")
        
        display_df = results_df[['æ’å', 'æ ‡é¢˜', 'ç»¼åˆè¯„åˆ†', 'ç›¸å…³æ€§è¯„åˆ†', 'å“è´¨è¯„åˆ†', 'æ¥æº']].copy()
        display_df['æ ‡é¢˜'] = display_df['æ ‡é¢˜'].str[:50]
        
        pd.set_option('display.max_colwidth', 50)
        pd.set_option('display.width', 150)
        print(display_df.to_string(index=False))
        
        print("\n\n" + "="*70)
        print("ğŸ† TOP 3 æœ€æœ‰åƒ¹å€¼çš„è³‡æ–™")
        print("="*70 + "\n")
        
        for i in range(min(3, len(results_df))):
            row = results_df.iloc[i]
            print(f"ã€ç¬¬ {i+1} åã€‘ç»¼åˆè¯„åˆ†: {row['ç»¼åˆè¯„åˆ†']}")
            print(f"æ ‡é¢˜: {row['æ ‡é¢˜']}")
            print(f"æ¥æº: {row['æ¥æº']}")
            print(f"è¯„åˆ†è¯¦æƒ…: ç›¸å…³æ€§ {row['ç›¸å…³æ€§è¯„åˆ†']:.1f} | å“è´¨ {row['å“è´¨è¯„åˆ†']:.1f}")
            print(f"ç½‘å€: {row['ç½‘å€']}")
            print(f"æ‘˜è¦: {row['æ‘˜è¦'][:120]}...")
            print("-" * 70 + "\n")
        
        export = input("æ˜¯å¦åŒ¯å‡ºçµæœåˆ°æª”æ¡ˆï¼Ÿ(y/n): ").strip().lower()
        if export == 'y':
            scorer.export_results(results_df)
        
        return results_df
    else:
        print("âŒ æœªèƒ½ç²å–æœ‰æ•ˆçµæœ")
        return None


if __name__ == "__main__":
    results = main()
    
    print("\n" + "="*70)
    print("ç¨‹å¼åŸ·è¡Œå®Œç•¢ï¼")
    print("="*70)
